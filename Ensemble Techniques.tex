	\chapter{Ensemble Techniques}
The most widely used ensemble techniques are Bagging, Boosting, and Stacking.

Bagging constructs model in parallel.  Boosting constructs models in series.

	\begin{bulletedlist}
		\item Bagging and Random Forest
		\begin{bulletedlist}
			\item Introduction to Ensemble techniques
			\item Introduction to Bagging
			\item Sampling with replacements
			\item Introduction to Random Forest
			\item Hand-on Bagging and Random Forest
		\end{bulletedlist}
		\item Boosting
		\begin{bulletedlist}
			\item Introduction to Boosting
			\item Boosting algorithms like:
			\item Adaboost
			\item Gradient boost
			\item XGBoost
			\item Hands-on Boosting
			\item Stacking
			\item Learning Material
		\end{bulletedlist}
	\end{bulletedlist}


	\section{Section FAQ}

	\resetquestioncounter{}
    \begin{qanda}
		\question{In Bagging, we can employ various models as long as the ensemble contains one particular type of algorithm (task)?}

		\answer{In bagging, we use the same algorithms and give different data points to obtain different models.  Bagging is homogenous therefore all the algorithms used have to be the same.  So suppose there is a bagging model with n\_estimators as 10 and if we use the Decision tree then all 10 algorithms have to be decision tree.  If any other algorithm supposes logistic regression then all 10 has to be logistic regression.  All the algorithms used in one bagging model have to be the same.  Part decision tree and part logistic regression algorithms won't work, all 10 have to be decision tree or all 10 have to be logistic regression.}
    \end{qanda}

    \begin{qanda}
		\question{How much difference between a model's train and test performance is considered as over fitting or under fitting?}

		\answer{There cannot be a universal threshold to decide the fit of a model. Under fit models are easy to identify but over fit models are relatively more difficult to identify as we can be sure of the model's fitness only when it is used in production. In practice, what matters is the actual test error, i.e. how often the predictions are wrong on new data. The limits on that are strictly a matter of business risk.
\newline{}

Theoretically, you can get any level of closeness you want between training and testing metrics, provided you have large enough sample space.
For example, if you are using a complex model like a deep neural network and have hundreds of thousands (or more) of observations, then you are entitled to see an average test error to be very close to an average training error, say well within 1 percent. But if you are using a simple model like linear regression or pruned decision tree and/or you only have a few observations, say in the range of hundreds, then average test error exceeding average training error by even 10 percent may not be unreasonable.
\newline{}

So it boils down to how much difference is acceptable to that domain or industry, then you have to increase the data size until the train-test difference is smaller than that.}
    \end{qanda}

    \begin{qanda}
	\question{To build each individual tree in random forest, whether a subset of only rows is taken or that of columns is also taken?}%%

	\answer{To prepare data for an individual tree, a subset of rows is taken but the entire set of features are available from the original data.  While splitting at the nodes, a random subset of features is selected. Hence both, row and column sampling is done in random forest.}
    \end{qanda}


    \begin{qanda}
	\question{I am getting the below error with Bagging Classifier with Logistic Regression as base\_estimator: AttributeError: `str' object has no attribute `decode' How to solve it?}

	\answer{The LogisticRegression function has a parameter named solver, which was earlier by default set to `liblinear', but after an update in sklearn, the default solver was changed to `lbfgs'. Kindly try setting the solver as liblinear, and try the code again, as shown below:
\newline{}

bagging\_lr = BaggingClassifier(base\_estimator=LogisticRegression(solver=`liblinear', random\_state=1), random\_state=1)
\newline{}

bagging\_lr.fit(X\_train,y\_train)}
    \end{qanda}

	\section{Bagging}
Bagging (parallel processing) splits the data into many smaller samples and creates a model from each one.  Even if one model over fits its particular data set, it cannot over fit the sample as a whole because it does not have access to it.

Bagging that uses decision trees as the models is called a random forrest.  In a random forrest, the models also only branch on the columns so that each model separated from the others (which have their own data split on the rows from the main data set).

Bagging is bootstrap (sampling the main data set) plus aggregation (putting all the models back together).

	\section{Random Forests}
While building:
	\begin{bulletedlist}
		\item Random sampling with replacement.
		\item For each subset, build a decision tree.  Use only $m<M$ randomly picked independent variables for each node's branching possibilities (only use a subset of the total number of columns).
		\item Do not prune.
	\end{bulletedlist}
If the number of selected column subsets ($m$) is too small, the ability of the models to predict is weak because the probability of selecting an important variable is small.  If $m$ is large, the models become correlated and predict the same thing.

While predicting.
	\begin{bulletedlist}
		\item Use each tree to make individual predictions.
		\item Combine predictions using voting.
		\begin{bulletedlist}
			\item Means for regression.
			\item Mode for classification.
		\end{bulletedlist}
	\end{bulletedlist}


	\section{Boosting}
	\resetquestioncounter{}
    \begin{qanda}
\question{What is the difference between AdaBoost and Gradient Boosting?}
\printinunitsof{in}\prntlen{\quandatextwidth}
\answer{
	\begin{tabular}{|p{0.5\quandatextwidth-2\tabcolsep}|p{0.5\quandatextwidth-2\tabcolsep}|} \hline
			\tablecolumnheadervlinesone{AdaBoost} & \tablecolumnheadervlinestwo{Gradient Boosting} \\ \hline
			Adaboost is more about ``voting weights'' &
            Gradient boosting is more about ``adding gradient optimization.'' \\ \hline
			Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model.	&
			Gradient boosting increases the accuracy by minimizing the Loss Function (an error which is the difference of actual and predicted value) and having this loss as a target for the next iteration. \\ \hline
			At each iteration, the Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. &
			Gradient boosting calculates the gradient (derivative) of the Loss Function with respect to the prediction (instead of the features). \newline

The Gradient boosting algorithm builds the first weak learner and calculates the Loss Function. \newline

It then builds a second learner to predict the loss after the first step. The step continues for the third learner and then for the fourth learner and so on until a certain threshold is reached. \\ \hline
		\end{tabular}
}
   \end{qanda}

    \begin{qanda}
\question{When to apply AdaBoost (Adaptive Boosting Algorithm)?}

\answer{Here is the list of all the key points below for an understanding of Adaboost:
	\begin{bulletedlist}
		\item AdaBoost can be applied to any classification algorithm, so it's really a technique that builds on top of other classifiers as opposed to being a classifier itself.
		\item You could just train a bunch of weak classifiers on your own and combine the results.
		\item There's really two things it figures out for you:
		\begin{bulletedlist}
			\item It helps you choose the training set for each new classifier that you train based on the results of the previous classifier.
			\item It determines how much weight should be given to each classifier's proposed answer when combining the results.
		\end{bulletedlist}
	\end{bulletedlist}
}
  \end{qanda}

	\begin{qanda}
\question{How does AdaBoost (Adaptive Boosting Algorithm) work?}
\answer{Important Points regarding working of Adaboost:
	\begin{bulletedlist}
		\item AdaBoost can be applied to any classification algorithm, so it's really a technique that builds on top of other classifiers as opposed to being a classifier itself.
		\item Each weak classifier should be trained on a random subset of the total training set.
		\item AdaBoost assigns a ``weight'' to each training example, which determines the probability that each example should appear in the training set.
		\begin{bulletedlist}
			\item The initial weights generally add up to 1.  For example, if there are 8 training examples, the weight assigned to each will be 1/8 initially.  All training examples have equal weights.
		\end{bulletedlist}
		\item If a training example is misclassified, then that training example is assigned a higher weight so that the probability of appearing that particular misclassified training example is higher in the next training set for training the classifier.
		\item After performing the previous step, ideally, the trained classifier will perform better on the misclassified examples next time.
		\item The weights are based on increasing the probability of being chosen in the next sequential training sub-set.
	\end{bulletedlist}
}
  \end{qanda}

    \begin{qanda}
\question{How to install the XGBoost library in Windows or Mac operations systems?}
\answer{For Windows, run the following command in your Jupyter notebook: \newline
\textcode{!pip install xgboost} \newline

For Mac, run the following command in your terminal: \newline
\textcode{conda install -c conda-forge xgboost} \newline

Note: Open a new terminal before using the above command in the Mac terminal. To open the terminal, please open the Launchpad and then click on the terminal icon.
}
  \end{qanda}

	\begin{qanda}
\question{The warning below appears while fitting XGBoost Classifier, how do you solve it?  \newline
\textcode{WARNING: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective `binary:logistic' was changed from `error' to `logloss'. Explicitly set eval\_metric if you'd like to restore the old behavior.}
}
\answer{To remove the warning try setting the eval\_metric hyperparameter as 'logloss', as shown below: \newline
\textcode{xgb = XGBClassifier(eval\_metric=`logloss')}
}
  \end{qanda}

	\subsection{Boosting Methods}
\noindent AdaBoosting (Adaptive Boosting)
	\begin{bulletedlist}
		\item In AdaBoost, the successive learners are created with a focus on the ill fitted data of the previous learner.
		\item Each successive learner focuses more and more on the harder to fit data i.e. their residuals in the previous tree
	\end{bulletedlist}

\vspace{\baselineskip}
\noindent Gradient Boosting
	\begin{bulletedlist}
		\item Each learner is fit on a modified version of original data. Original data is replaced with the x
values and residuals from previous learner
		\item By fitting new models to the residuals, the overall learner gradually improves in areas
where residuals are initially high
	\end{bulletedlist}

\vspace{\baselineskip}
\noindent XG Boost (Extreme Gradient Boosting)
	\begin{bulletedlist}
		\item Upgraded implementation of Gradient Boosting. Developed for high computational speed, scalability, and better performance.
		\item Parallel Implementation, Cross-Validation, Cache Optimization, Distributed Computation
	\end{bulletedlist}

	\section{Comparison}
What is the difference between bagging and boosting?

	\noindent \begin{tabular}{|p{0.5\textwidth-2\tabcolsep}|p{0.5\textwidth-2\tabcolsep}|} \hline
		\tablecolumnheadervlinesone{Bagging} & \tablecolumnheadervlinestwo{Boosting} \\ \hline
		Parallel Training - All the weak learners are built in parallel i.e. independent of each other. &
		Sequential Training - Successive weak learners to improve the accuracy from the prior learners \\ \hline
		Equal weightage - Each weak learner has equal weight in the final prediction. &
		Weighted average - More weight to those weak learners with better performance. \\ \hline
		Independent samples - Samples are drawn from the original data set with replacement to train each individual weak learner. &
		Dependent samples - Subsequent samples have more of those observations which had relatively higher errors in previous weak learners. \\ \hline
		Can help reduce variance of the model. &
		Can help reduce bias of the model. \\ \hline
		Examples: Bagging Classifier, Random Forest &
		Examples are AdaBoost, Gradient Boosting Classifier \\ \hline
	\end{tabular}

	\section{Gradient Boosting}
Gradient boosting uses subsequent models to predict the residuals.  The original model is added to the results of the residual predictors (which weight errors more) to get a complete model.  The residual predictors are added to the main model using a learning correction ($\alpha$) that reduces its contribution.  If a weight of 1 is used, the model will be over fit.

	\section{XG Boost (Extreme Gradient Boosting)}
A highly efficient gradient boosting algorithm.  It has some mathematical techniques and a very good implementation.

	\begin{bulletedlist}
		\item XGBoost builds upon the idea of gradient boosting algorithm with some modifications. Gradient boosted trees are built in sequence because each estimator predicts residuals of the previous estimator, which makes it slow at the time of model training as compared to building estimators in parallel.
		\item Thus, the main concentration in XGBoost is speed enhancement and model performance.
		\item The speed and scalability of XGBoost is due to several important features that help in better computing using concepts like parallelization, cache optimization, out of core computing and
distributed computing.
	\end{bulletedlist}

	\subsection{Salient features}
	\begin{bulletedlist}
		\item \textbf{Parallelization} - In XGBoost, data is stored in in-memory units, called block, and the optimal split in each tree can be found in parallel using this block structure.  This reduces the time of model training significantly.
		\item \textbf{Cache Optimization} - XGBoost optimizes the block size for efficient parallelization and make best use of hardware.
		\item \textbf{Out-of-Core Computing} - This helps to handle huge data sets which do not even fit into memory.
		\item \textbf{Distributed Computing} - It helps to train huge models using multiple similar machines.
		\item \textbf{Missing Values Imputation} - XGBoost is designed to handle missing values internally.  It uses a default direction for the missing values. However, the default
direction can be right-child or left-child, and it is learned in the tree construction process to choose the best direction that optimizes the training loss.
	\end{bulletedlist}